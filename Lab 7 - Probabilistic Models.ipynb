{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c892250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    \"I love cats . cats are cute pets.\",\n",
    "    \"Dogs are loyal. Dogs are good friends.\",\n",
    "    \"Birds can sing. Birds fly in the sky.\",\n",
    "    \"Fish live underwater. Fish come in many colors.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a685610",
   "metadata": {},
   "source": [
    "In this section, we define and create unigram models for the documents. Unigrams are single words or terms, and a unigram model represents the probability distribution of individual terms in the document. The unigram_model function counts the occurrences of each term in a document, calculates the probabilities, and returns the unigram model. We create unigram models for all documents in the collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d0e623",
   "metadata": {},
   "source": [
    "# Create Unigram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d589155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_model(document):\n",
    "    words = document.split()\n",
    "    total_words = len(words)\n",
    "    unigram_counts = defaultdict(int)\n",
    "    for word in words:\n",
    "        unigram_counts[word] += 1\n",
    "    unigram_model = {word: count / total_words for word, count in unigram_counts.items()}\n",
    "    return unigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9c571b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unigram models for all documents\n",
    "unigram_models = [unigram_model(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a16bdec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'I': 0.125,\n",
       "  'love': 0.125,\n",
       "  'cats': 0.25,\n",
       "  '.': 0.125,\n",
       "  'are': 0.125,\n",
       "  'cute': 0.125,\n",
       "  'pets.': 0.125},\n",
       " {'Dogs': 0.2857142857142857,\n",
       "  'are': 0.2857142857142857,\n",
       "  'loyal.': 0.14285714285714285,\n",
       "  'good': 0.14285714285714285,\n",
       "  'friends.': 0.14285714285714285},\n",
       " {'Birds': 0.25,\n",
       "  'can': 0.125,\n",
       "  'sing.': 0.125,\n",
       "  'fly': 0.125,\n",
       "  'in': 0.125,\n",
       "  'the': 0.125,\n",
       "  'sky.': 0.125},\n",
       " {'Fish': 0.25,\n",
       "  'live': 0.125,\n",
       "  'underwater.': 0.125,\n",
       "  'come': 0.125,\n",
       "  'in': 0.125,\n",
       "  'many': 0.125,\n",
       "  'colors.': 0.125}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "faa6e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have a query \n",
    "query = \"I like cats and dogs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85f84e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_query_probability(query, document_model):\n",
    "    # Tokenize the query into words\n",
    "    query_words = query.split()\n",
    "    \n",
    "    # Initialize the probability for the entire query\n",
    "    query_probability = 1.0\n",
    "    \n",
    "    # Calculate the probability for each term in the query\n",
    "    for word in query_words:\n",
    "        if word in document_model:\n",
    "            query_probability *= document_model[word]\n",
    "        else:\n",
    "            query_probability = 0.0\n",
    "            break\n",
    "    \n",
    "    return query_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25934062",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_probability = calculate_query_probability(query, unigram_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e57e1768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2641c2c",
   "metadata": {},
   "source": [
    "## Your task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "771db8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Laplace Smoothing for this problem\n",
    "\n",
    "# https://www.exploredatabase.com/2020/10/explain-add-1-laplace-smoothing-with-example.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a7f2705d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\MSI\\Documents\\GitHub\\lab-7-probabilistic-model-Yongyi12\\Lab 7 - Probabilistic Models.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MSI/Documents/GitHub/lab-7-probabilistic-model-Yongyi12/Lab%207%20-%20Probabilistic%20Models.ipynb#X15sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mI like cats and dogs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MSI/Documents/GitHub/lab-7-probabilistic-model-Yongyi12/Lab%207%20-%20Probabilistic%20Models.ipynb#X15sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Calculate the query probability using the bigram model with Laplace smoothing\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/MSI/Documents/GitHub/lab-7-probabilistic-model-Yongyi12/Lab%207%20-%20Probabilistic%20Models.ipynb#X15sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m query_probability \u001b[39m=\u001b[39m calculate_query_probability_with_smoothing(query, bigram_model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MSI/Documents/GitHub/lab-7-probabilistic-model-Yongyi12/Lab%207%20-%20Probabilistic%20Models.ipynb#X15sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Print the query probability\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MSI/Documents/GitHub/lab-7-probabilistic-model-Yongyi12/Lab%207%20-%20Probabilistic%20Models.ipynb#X15sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mQuery Probability:\u001b[39m\u001b[39m\"\u001b[39m, query_probability)\n",
      "\u001b[1;32mc:\\Users\\MSI\\Documents\\GitHub\\lab-7-probabilistic-model-Yongyi12\\Lab 7 - Probabilistic Models.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MSI/Documents/GitHub/lab-7-probabilistic-model-Yongyi12/Lab%207%20-%20Probabilistic%20Models.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(query_words) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MSI/Documents/GitHub/lab-7-probabilistic-model-Yongyi12/Lab%207%20-%20Probabilistic%20Models.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     word1, word2 \u001b[39m=\u001b[39m query_words[i], query_words[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/MSI/Documents/GitHub/lab-7-probabilistic-model-Yongyi12/Lab%207%20-%20Probabilistic%20Models.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mif\u001b[39;00m word1 \u001b[39min\u001b[39;00m bigram_model \u001b[39mand\u001b[39;00m word2 \u001b[39min\u001b[39;00m bigram_model[word1]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MSI/Documents/GitHub/lab-7-probabilistic-model-Yongyi12/Lab%207%20-%20Probabilistic%20Models.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         query_probability \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m bigram_model[word1][word2]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MSI/Documents/GitHub/lab-7-probabilistic-model-Yongyi12/Lab%207%20-%20Probabilistic%20Models.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to calculate the probability of a query using the bigram model with Laplace smoothing\n",
    "def calculate_query_probability_with_smoothing(query, bigram_model):\n",
    "    query_words = query.split()\n",
    "    query_probability = 1.0\n",
    "\n",
    "    for i in range(len(query_words) - 1):\n",
    "        word1, word2 = query_words[i], query_words[i + 1]\n",
    "        if word1 in bigram_model and word2 in bigram_model[word1]:\n",
    "            query_probability *= bigram_model[word1][word2]\n",
    "        else:\n",
    "            query_probability = 0.0\n",
    "            break\n",
    "\n",
    "    return query_probability\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    \"I love cats . cats are cute pets.\",\n",
    "    \"Dogs are loyal. Dogs are good friends.\",\n",
    "    \"Birds can sing. Birds fly in the sky.\",\n",
    "    \"Fish live underwater. Fish come in many colors.\"\n",
    "]\n",
    "\n",
    "# Function to create a bigram model with Laplace smoothing\n",
    "def bigram_model_with_smoothing(documents):\n",
    "    # Implement the function to create a bigram model with Laplace smoothing\n",
    "    pass\n",
    "\n",
    "# Create a bigram model with Laplace smoothing\n",
    "bigram_model = bigram_model_with_smoothing(documents)\n",
    "\n",
    "# Define the query\n",
    "query = \"I like cats and dogs\"\n",
    "\n",
    "# Calculate the query probability using the bigram model with Laplace smoothing\n",
    "query_probability = calculate_query_probability_with_smoothing(query, bigram_model)\n",
    "\n",
    "# Print the query probability\n",
    "print(\"Query Probability:\", query_probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714b7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word count: 31\n"
     ]
    }
   ],
   "source": [
    "# Define the documents\n",
    "documents = [\n",
    "    \"I love cats . cats are cute pets.\",\n",
    "    \"Dogs are loyal. Dogs are good friends.\",\n",
    "    \"Birds can sing. Birds fly in the sky.\",\n",
    "    \"Fish live underwater. Fish come in many colors.\"\n",
    "]\n",
    "\n",
    "# Function to count the total number of words in all documents\n",
    "def count_total_words(documents):\n",
    "    total_words = 0\n",
    "    for doc in documents:\n",
    "        total_words += len(doc.split())\n",
    "    return total_words\n",
    "\n",
    "# Count the total number of words in all documents\n",
    "total_word_count = count_total_words(documents)\n",
    "\n",
    "# Print the total word count\n",
    "print(f\"Total word count: {total_word_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
